{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqXm1x-SJvNL",
        "outputId": "dce12de0-9126-4a89-a9dc-9c5537aebf0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing monolith_inspector.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile monolith_inspector.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Monolith Inspector for Java (jPetStore-ready)\n",
        "\n",
        "What it does\n",
        "------------\n",
        "- Clones a repo (optional) OR scans a local path\n",
        "- Emits:\n",
        "  out/\n",
        "    file_structure.md\n",
        "    file_structure.json\n",
        "    knowledge_graph.graphml\n",
        "    knowledge_graph.json\n",
        "    coupling_metrics.csv   # package-level afferent/efferent coupling\n",
        "\n",
        "Graph nodes\n",
        "-----------\n",
        "- package: com.example.foo\n",
        "- class:   com.example.foo.Bar\n",
        "- method:  com.example.foo.Bar#baz(argTypes)\n",
        "\n",
        "Graph edges (MultiDiGraph, directed)\n",
        "------------------------------------\n",
        "- package -> class                  (contains)\n",
        "- class   -> class                  (extends / implements)\n",
        "- class   -> class/package          (import depends-on)\n",
        "- class   -> method                 (defines)\n",
        "- class   -> class/method?          (calls – best-effort via javalang)\n",
        "\n",
        "Notes\n",
        "-----\n",
        "- Java resolution of method calls is heuristic (no type solver). Still great for macro structure.\n",
        "- Safe on large repos; skips generated/build dirs.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import javalang\n",
        "import networkx as nx\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "\n",
        "SKIP_DIRS = {\n",
        "    \".git\", \".idea\", \".vscode\", \"target\", \"build\", \"out\", \".gradle\", \".mvn\",\n",
        "    \"node_modules\"\n",
        "}\n",
        "\n",
        "JAVA_EXT = {\".java\"}\n",
        "\n",
        "def run(cmd, cwd=None) -> str:\n",
        "    p = subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed: {' '.join(cmd)}\\n{p.stdout}\")\n",
        "    return p.stdout\n",
        "\n",
        "def maybe_clone(repo, branch, dest) -> Path:\n",
        "    dest = Path(dest).expanduser().resolve()\n",
        "    if dest.exists() and any(dest.iterdir()):\n",
        "        return dest\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    run([\"git\", \"clone\", \"--depth\", \"1\", \"-b\", branch, repo, str(dest)])\n",
        "    return dest\n",
        "\n",
        "def walk_files(root: Path):\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        # prune skip dirs in-place\n",
        "        dirnames[:] = [d for d in dirnames if d not in SKIP_DIRS and not d.startswith(\".\")]\n",
        "        for f in filenames:\n",
        "            p = Path(dirpath) / f\n",
        "            if p.suffix.lower() in JAVA_EXT:\n",
        "                yield p\n",
        "\n",
        "def build_tree_markdown(root: Path) -> str:\n",
        "    \"\"\"Return a Markdown tree (like tree) but light-weight.\"\"\"\n",
        "    lines = [f\"# File Structure for {root.name}\\n\"]\n",
        "\n",
        "    def relpath(p): return str(p.relative_to(root))\n",
        "\n",
        "    def tree(prefix: str, path: Path):\n",
        "        entries = sorted([*path.iterdir()], key=lambda x: (x.is_file(), x.name.lower()))\n",
        "        for i, e in enumerate(entries):\n",
        "            if e.name in SKIP_DIRS or e.name.startswith(\".\"):\n",
        "                continue\n",
        "            connector = \"└─\" if i == len(entries)-1 else \"├─\"\n",
        "            lines.append(f\"{prefix}{connector} {e.name}\")\n",
        "            if e.is_dir():\n",
        "                child_prefix = f\"{prefix}{'   ' if i == len(entries)-1 else '│  '}\"\n",
        "                tree(child_prefix, e)\n",
        "\n",
        "    tree(\"\", root)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def tree_to_json(root: Path):\n",
        "    def node(path: Path):\n",
        "        if path.is_dir():\n",
        "            return {\n",
        "                \"name\": path.name,\n",
        "                \"type\": \"dir\",\n",
        "                \"children\": [\n",
        "                    node(c) for c in sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))\n",
        "                    if c.name not in SKIP_DIRS and not c.name.startswith(\".\")\n",
        "                ]\n",
        "            }\n",
        "        else:\n",
        "            return {\"name\": path.name, \"type\": \"file\"}\n",
        "    return node(root)\n",
        "\n",
        "# ----------------------------\n",
        "# Java parsing (javalang)\n",
        "# ----------------------------\n",
        "\n",
        "def parse_java(path: Path):\n",
        "    try:\n",
        "        src = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        tree = javalang.parse.parse(src)\n",
        "        return src, tree\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def locate_package(tree) -> str:\n",
        "    try:\n",
        "        return tree.package.name if tree.package else \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# from typing import List\n",
        "def short_to_fqcn(short: str, pkg: str, imports: List[str]) -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Best effort: if short is fully qualified, keep it.\n",
        "    If not, try imports; else qualify with current package.\n",
        "    \"\"\"\n",
        "    if \".\" in short and short[0].islower() is False:  # looks like fqcn (e.g., com.foo.Bar)\n",
        "        return short\n",
        "    base = short.split(\".\")[0]\n",
        "    for imp in imports:\n",
        "        if imp.endswith(\".\" + base) or imp.endswith(\".\" + base + \".*\"):\n",
        "            # if wildcard import, we can't pin class – return package-level\n",
        "            if imp.endswith(\".*\"):\n",
        "                return imp[:-2] + \".\" + base\n",
        "            return imp\n",
        "    return f\"{pkg}.{base}\" if pkg else base\n",
        "\n",
        "def method_sig(pkg: str, cls: str, m) -> str:\n",
        "    # com.pets.Store#find(String,int)\n",
        "    params = \",\".join([getattr(p.type, \"name\", \"Object\") for p in (m.parameters or [])])\n",
        "    return f\"{pkg}.{cls}#{m.name}({params})\" if pkg else f\"{cls}#{m.name}({params})\"\n",
        "\n",
        "# ----------------------------\n",
        "# Graph construction\n",
        "# ----------------------------\n",
        "\n",
        "def analyze_repo(root: Path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - graph (MultiDiGraph)\n",
        "      - file_index: list of java files processed\n",
        "      - package_coupling: dict of pkg -> {\"afferent\": set, \"efferent\": set}\n",
        "    \"\"\"\n",
        "    G = nx.MultiDiGraph()\n",
        "    file_index = []\n",
        "    package_coupling = defaultdict(lambda: {\"afferent\": set(), \"efferent\": set()})\n",
        "\n",
        "    for jf in walk_files(root):\n",
        "        src, tree = parse_java(jf)\n",
        "        if tree is None:\n",
        "            continue\n",
        "        file_index.append(str(jf.relative_to(root)))\n",
        "\n",
        "        pkg = locate_package(tree)\n",
        "        imports = [imp.path for imp in (tree.imports or []) if hasattr(imp, \"path\")]\n",
        "        # add package node\n",
        "        if pkg:\n",
        "            G.add_node(f\"package:{pkg}\", kind=\"package\", name=pkg)\n",
        "\n",
        "        # collect class/interface declarations\n",
        "        types = [t for t in tree.types if hasattr(t, \"name\")]\n",
        "        for t in types:\n",
        "            cls_name = t.name\n",
        "            fqcn = f\"{pkg}.{cls_name}\" if pkg else cls_name\n",
        "            G.add_node(f\"class:{fqcn}\", kind=\"class\", name=fqcn, file=str(jf.relative_to(root)))\n",
        "\n",
        "            # package contains class\n",
        "            if pkg:\n",
        "                G.add_edge(f\"package:{pkg}\", f\"class:{fqcn}\", kind=\"contains\")\n",
        "\n",
        "            # extends/implements edges\n",
        "            if getattr(t, \"extends\", None):\n",
        "                base = getattr(t.extends, \"name\", None) or str(t.extends)\n",
        "                super_fq = short_to_fqcn(base, pkg, imports)\n",
        "                G.add_node(f\"class:{super_fq}\", kind=\"class\", name=super_fq)\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"class:{super_fq}\", kind=\"extends\")\n",
        "\n",
        "                # coupling: class pkg depends on super pkg\n",
        "                super_pkg = \".\".join(super_fq.split(\".\")[:-1])\n",
        "                if pkg and super_pkg and super_pkg != pkg:\n",
        "                    package_coupling[pkg][\"efferent\"].add(super_pkg)\n",
        "                    package_coupling[super_pkg][\"afferent\"].add(pkg)\n",
        "\n",
        "            impls = getattr(t, \"implements\", []) or []\n",
        "            for impl in impls:\n",
        "                iname = getattr(impl, \"name\", None) or str(impl)\n",
        "                int_fq = short_to_fqcn(iname, pkg, imports)\n",
        "                G.add_node(f\"class:{int_fq}\", kind=\"class\", name=int_fq)\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"class:{int_fq}\", kind=\"implements\")\n",
        "\n",
        "                int_pkg = \".\".join(int_fq.split(\".\")[:-1])\n",
        "                if pkg and int_pkg and int_pkg != pkg:\n",
        "                    package_coupling[pkg][\"efferent\"].add(int_pkg)\n",
        "                    package_coupling[int_pkg][\"afferent\"].add(pkg)\n",
        "\n",
        "            # import edges (package-level dependency)\n",
        "            for imp in imports:\n",
        "                # if import is a class, link class->that class; also package coupling\n",
        "                imported = imp\n",
        "                G.add_node(f\"class:{imported}\", kind=\"class\", name=imported)\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"class:{imported}\", kind=\"imports\")\n",
        "\n",
        "                imp_pkg = \".\".join(imported.split(\".\")[:-1]) if \".\" in imported else \"\"\n",
        "                if pkg and imp_pkg and imp_pkg != pkg and not imported.endswith(\".*\"):\n",
        "                    package_coupling[pkg][\"efferent\"].add(imp_pkg)\n",
        "                    package_coupling[imp_pkg][\"afferent\"].add(pkg)\n",
        "\n",
        "            # methods + definitions\n",
        "            methods = [m for m in getattr(t, \"methods\", [])]\n",
        "            for m in methods:\n",
        "                msig = method_sig(pkg, cls_name, m)\n",
        "                G.add_node(f\"method:{msig}\", kind=\"method\", name=msig, file=str(jf.relative_to(root)))\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"method:{msig}\", kind=\"defines\")\n",
        "\n",
        "                # method calls (best-effort)\n",
        "                # javalang gives MethodInvocation with .qualifier and .member\n",
        "                # We’ll edge from class -> (guessed) target class or method token\n",
        "                body = m.body or []\n",
        "                try:\n",
        "                    # flatten nodes\n",
        "                    for path, node in m.filter(javalang.tree.MethodInvocation):\n",
        "                        qual = node.qualifier  # may be a class/variable name\n",
        "                        member = node.member\n",
        "                        target = None\n",
        "\n",
        "                        if qual:\n",
        "                            target = short_to_fqcn(qual, pkg, imports)\n",
        "                            G.add_node(f\"class:{target}\", kind=\"class\", name=target)\n",
        "                            G.add_edge(f\"class:{fqcn}\", f\"class:{target}\", kind=\"calls\")\n",
        "\n",
        "                            # package coupling for calls\n",
        "                            tgt_pkg = \".\".join(target.split(\".\")[:-1])\n",
        "                            if pkg and tgt_pkg and tgt_pkg != pkg:\n",
        "                                package_coupling[pkg][\"efferent\"].add(tgt_pkg)\n",
        "                                package_coupling[tgt_pkg][\"afferent\"].add(pkg)\n",
        "                        else:\n",
        "                            # unknown receiver – record as method token under this class namespace\n",
        "                            token = f\"{pkg}.{member}\" if pkg else member\n",
        "                            G.add_node(f\"method:{token}\", kind=\"method\", name=token)\n",
        "                            G.add_edge(f\"class:{fqcn}\", f\"method:{token}\", kind=\"calls\")\n",
        "\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    return G, file_index, package_coupling\n",
        "\n",
        "# ----------------------------\n",
        "# Outputs\n",
        "# ----------------------------\n",
        "\n",
        "def write_outputs(root: Path, outdir: Path, G: nx.MultiDiGraph, file_index, package_coupling):\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # file structure\n",
        "    md = build_tree_markdown(root)\n",
        "    (outdir / \"file_structure.md\").write_text(md, encoding=\"utf-8\")\n",
        "    struct_json = tree_to_json(root)\n",
        "    (outdir / \"file_structure.json\").write_text(json.dumps(struct_json, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # graph\n",
        "    nx.write_graphml(G, outdir / \"knowledge_graph.graphml\")\n",
        "\n",
        "    # json dump\n",
        "    nodes = [{\"id\": n, **G.nodes[n]} for n in G.nodes]\n",
        "    edges = [{\"u\": u, \"v\": v, \"key\": k, **G.edges[u, v, k]} for u, v, k in G.edges(keys=True)]\n",
        "    (outdir / \"knowledge_graph.json\").write_text(json.dumps({\"nodes\": nodes, \"edges\": edges}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # coupling metrics\n",
        "    import csv\n",
        "    with (outdir / \"coupling_metrics.csv\").open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"package\", \"afferent_count\", \"efferent_count\", \"afferent_set\", \"efferent_set\"])\n",
        "        for pkg, ce in sorted(package_coupling.items()):\n",
        "            aff = sorted(ce[\"afferent\"])\n",
        "            eff = sorted(ce[\"efferent\"])\n",
        "            w.writerow([pkg, len(aff), len(eff), \";\".join(aff), \";\".join(eff)])\n",
        "\n",
        "    # tiny console summary\n",
        "    print(f\"[OK] Wrote outputs to: {outdir}\")\n",
        "    print(f\" - file_structure.md / .json\")\n",
        "    print(f\" - knowledge_graph.graphml / .json\")\n",
        "    print(f\" - coupling_metrics.csv\")\n",
        "    print(f\"Java files parsed: {len(file_index)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# CLI\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Parse Java monolith (like jPetStore) and build a knowledge graph.\")\n",
        "    ap.add_argument(\"--repo\", help=\"Git repo URL (optional). If omitted, use --path.\", default=None)\n",
        "    ap.add_argument(\"--branch\", help=\"Branch to clone\", default=\"master\")\n",
        "    ap.add_argument(\"--path\", help=\"Local path to repo (if you already cloned).\", default=None)\n",
        "    ap.add_argument(\"--out\", help=\"Output directory\", default=\"out\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    if not args.repo and not args.path:\n",
        "        ap.error(\"Provide either --repo or --path\")\n",
        "\n",
        "    if args.repo:\n",
        "        root = maybe_clone(args.repo, args.branch, \"./_repo_checkout\")\n",
        "    else:\n",
        "        root = Path(args.path).expanduser().resolve()\n",
        "        if not root.exists():\n",
        "            raise SystemExit(f\"Path not found: {root}\")\n",
        "\n",
        "    G, files, coupling = analyze_repo(root)\n",
        "    write_outputs(root, Path(args.out), G, files, coupling)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f12031d",
        "outputId": "3f7c7e5a-a1c4-45d5-9ade-9e9be2bbdf2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting javalang\n",
            "  Downloading javalang-0.13.0-py3-none-any.whl.metadata (805 bytes)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from javalang) (1.17.0)\n",
            "Downloading javalang-0.13.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: javalang\n",
            "Successfully installed javalang-0.13.0\n"
          ]
        }
      ],
      "source": [
        "%pip install javalang networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a3e9df6",
        "outputId": "c142d00d-c534-4a95-9967-48f3f90a48db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script executed successfully!\n",
            "Stdout:\n",
            " [OK] Wrote outputs to: analysis_output\n",
            " - file_structure.md / .json\n",
            " - knowledge_graph.graphml / .json\n",
            " - coupling_metrics.csv\n",
            "Java files parsed: 71\n",
            "\n",
            "Stderr:\n",
            " \n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Create a directory for the output\n",
        "output_dir = \"analysis_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Run the script with a sample repo (jPetStore)\n",
        "# Assuming the script is saved as 'monolith_inspector.py' in the current directory\n",
        "script_name = \"monolith_inspector.py\"\n",
        "\n",
        "# The script content is already in monolith_inspector.py due to the %%writefile magic command\n",
        "\n",
        "command = [\"python3\", script_name, \"--repo\", \"https://github.com/stleary/JSON-java\", \"--out\", output_dir]\n",
        "\n",
        "try:\n",
        "    process = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "    print(\"Script executed successfully!\")\n",
        "    print(\"Stdout:\\n\", process.stdout)\n",
        "    print(\"Stderr:\\n\", process.stderr)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error executing script: {e}\")\n",
        "    print(\"Stdout:\\n\", e.stdout)\n",
        "    print(\"Stderr:\\n\", e.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8e75ebc",
        "outputId": "a0af1485-920f-4948-dc81-ef1f8c65e8b1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Graph JSON structure:\n",
            "Keys: dict_keys(['nodes', 'edges'])\n",
            "\n",
            "First 5 Nodes:\n",
            "{'id': 'package:org.json.junit', 'kind': 'package', 'name': 'org.json.junit'}\n",
            "{'id': 'class:org.json.junit.Util', 'kind': 'class', 'name': 'org.json.junit.Util', 'file': 'src/test/java/org/json/junit/Util.java'}\n",
            "{'id': 'class:org.junit.Assert', 'kind': 'class', 'name': 'org.junit.Assert'}\n",
            "{'id': 'class:java.util', 'kind': 'class', 'name': 'java.util'}\n",
            "{'id': 'class:org.json', 'kind': 'class', 'name': 'org.json'}\n",
            "\n",
            "First 5 Edges:\n",
            "{'u': 'package:org.json.junit', 'v': 'class:org.json.junit.Util', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.json.junit', 'v': 'class:org.json.junit.CookieTest', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.json.junit', 'v': 'class:org.json.junit.XMLTokenerTest', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.json.junit', 'v': 'class:org.json.junit.XMLConfigurationTest', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.json.junit', 'v': 'class:org.json.junit.JSONStringTest', 'key': 0, 'kind': 'contains'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "output_dir = \"analysis_output\"\n",
        "graph_json_path = os.path.join(output_dir, \"knowledge_graph.json\")\n",
        "\n",
        "if os.path.exists(graph_json_path):\n",
        "    with open(graph_json_path, 'r') as f:\n",
        "        graph_data = json.load(f)\n",
        "    # Displaying the full JSON might be too large, so display a part of it or a summary\n",
        "    # For simplicity, let's display the keys and the first few items of nodes and edges\n",
        "    print(\"Knowledge Graph JSON structure:\")\n",
        "    print(f\"Keys: {graph_data.keys()}\")\n",
        "    print(\"\\nFirst 5 Nodes:\")\n",
        "    for i, node in enumerate(graph_data.get('nodes', [])[:5]):\n",
        "        print(node)\n",
        "    print(\"\\nFirst 5 Edges:\")\n",
        "    for i, edge in enumerate(graph_data.get('edges', [])[:5]):\n",
        "        print(edge)\n",
        "\n",
        "else:\n",
        "    print(f\"Knowledge graph JSON file not found at: {graph_json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTnSIa3YMO-",
        "outputId": "3ef6479f-3c08-49de-c475-3081bd8eb5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing graphrag_merger.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile graphrag_merger.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GraphRAG Merger\n",
        "\n",
        "Takes the structural knowledge graph (from monolith_inspector.py)\n",
        "and enriches it with semantic/functional context for RAG-style queries.\n",
        "\n",
        "Inputs\n",
        "------\n",
        "- knowledge_graph.json (nodes + edges from networkx export)\n",
        "- file_structure.json  (hierarchical tree)\n",
        "- coupling_metrics.csv (afferent/efferent coupling)\n",
        "\n",
        "Outputs\n",
        "-------\n",
        "out/\n",
        "  graph_context.json   # GraphRAG-compatible schema (entities + relationships)\n",
        "  node_index.json      # index with text chunks + embeddings\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import hashlib\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "\n",
        "def read_json(path: Path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def read_csv(path: Path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for row in csv.DictReader(f):\n",
        "            rows.append(row)\n",
        "    return rows\n",
        "\n",
        "def simple_embed(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Placeholder: simple hash-based embedding\n",
        "    (replace with OpenAI, HuggingFace, etc. if needed).\n",
        "    \"\"\"\n",
        "    h = hashlib.sha256(text.encode(\"utf-8\")).digest()\n",
        "    return [x/255 for x in h[:32]]  # 32-dim fake embedding\n",
        "\n",
        "# ----------------------------\n",
        "# Build GraphRAG schema\n",
        "# ----------------------------\n",
        "\n",
        "def build_graphrag_context(graph_data: Dict[str, Any],\n",
        "                           file_struct: Dict[str, Any],\n",
        "                           coupling: List[Dict[str, str]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convert nodes + edges + file context into GraphRAG style:\n",
        "      {\n",
        "        \"entities\": [ {id, type, name, text, embedding} ],\n",
        "        \"relationships\": [ {source, target, type} ]\n",
        "      }\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    relationships = []\n",
        "\n",
        "    # Convert nodes\n",
        "    for n in graph_data[\"nodes\"]:\n",
        "        node_id = n[\"id\"]\n",
        "        node_type = n.get(\"kind\", \"unknown\")\n",
        "        name = n.get(\"name\", node_id)\n",
        "\n",
        "        text = f\"{node_type} {name}\"\n",
        "        if \"file\" in n:\n",
        "            text += f\" defined in {n['file']}\"\n",
        "\n",
        "        entities.append({\n",
        "            \"id\": node_id,\n",
        "            \"type\": node_type,\n",
        "            \"name\": name,\n",
        "            \"text\": text,\n",
        "            \"embedding\": simple_embed(text)\n",
        "        })\n",
        "\n",
        "    # Convert edges\n",
        "    for e in graph_data[\"edges\"]:\n",
        "        relationships.append({\n",
        "            \"source\": e[\"u\"],\n",
        "            \"target\": e[\"v\"],\n",
        "            \"type\": e.get(\"kind\", \"related\")\n",
        "        })\n",
        "\n",
        "    # Enrich with coupling as extra edges\n",
        "    for row in coupling:\n",
        "        pkg = row[\"package\"]\n",
        "        for eff in row[\"efferent_set\"].split(\";\"):\n",
        "            if eff:\n",
        "                relationships.append({\n",
        "                    \"source\": f\"package:{pkg}\",\n",
        "                    \"target\": f\"package:{eff}\",\n",
        "                    \"type\": \"couples-to\"\n",
        "                })\n",
        "\n",
        "    return {\"entities\": entities, \"relationships\": relationships}\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "\n",
        "def main(outdir=\"analysis_output\"):\n",
        "    out = Path(outdir)\n",
        "    graph_data = read_json(out / \"knowledge_graph.json\")\n",
        "    file_struct = read_json(out / \"file_structure.json\")\n",
        "    coupling = read_csv(out / \"coupling_metrics.csv\")\n",
        "\n",
        "    merged = build_graphrag_context(graph_data, file_struct, coupling)\n",
        "\n",
        "    (out / \"graph_context.json\").write_text(\n",
        "        json.dumps(merged, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    # Build node index for retrieval\n",
        "    node_index = {\n",
        "        e[\"id\"]: {\"text\": e[\"text\"], \"embedding\": e[\"embedding\"]}\n",
        "        for e in merged[\"entities\"]\n",
        "    }\n",
        "    (out / \"node_index.json\").write_text(\n",
        "        json.dumps(node_index, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "    print(f\"[OK] GraphRAG context written to {out/'graph_context.json'}\")\n",
        "    print(f\"[OK] Node index written to {out/'node_index.json'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbz6g7t7Yh-u",
        "outputId": "1f44ffb8-0e74-4f04-ce8c-9b958fe66cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] GraphRAG context written to analysis_output/graph_context.json\n",
            "[OK] Node index written to analysis_output/node_index.json\n"
          ]
        }
      ],
      "source": [
        "!python3 graphrag_merger.py --out out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tBMONB7SiB37"
      },
      "outputs": [],
      "source": [
        "# !mkdir -p models\n",
        "# !wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O tinyllama.gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "import os\n",
        "\n",
        "# Replace with your actual API key\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyB3II7ESGsgzPcHZ440X4MLoHkJdPXirG0\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkYMImWl70yA",
        "outputId": "2006feb6-253a-4b8b-e9b1-cb0f93cfd360"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "paRKmuvaeeL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c0a57a-9ed5-438c-bdf1-8635cf9023f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full API response: {\n",
            "  \"candidates\": [\n",
            "    {\n",
            "      \"content\": {\n",
            "        \"parts\": [\n",
            "          {\n",
            "            \"text\": \"```json\\n{\\n  \\\"microservices\\\": [\\n    {\\n      \\\"name\\\": \\\"CoreService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Provides core JSON parsing and manipulation functionalities.\\\",\\n        \\\"Handles basic JSON object and array creation, parsing and validation.\\\",\\n        \\\"Manages JSON tokens.\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"CDL.java\\\",\\n        \\\"Cookie.java\\\",\\n        \\\"CookieList.java\\\",\\n        \\\"HTTP.java\\\",\\n        \\\"HTTPTokener.java\\\",\\n        \\\"JSONArray.java\\\",\\n        \\\"JSONException.java\\\",\\n        \\\"JSONObject.java\\\",\\n        \\\"JSONString.java\\\",\\n        \\\"JSONStringer.java\\\",\\n        \\\"JSONTokener.java\\\",\\n        \\\"JSONWriter.java\\\",\\n        \\\"Property.java\\\",\\n        \\\"StringBuilderWriter.java\\\",\\n        \\\"XML.java\\\",\\n        \\\"XMLTokener.java\\\",\\n        \\\"XMLXsiTypeConverter.java\\\",\\n        \\\"ParserConfiguration.java\\\",\\n        \\\"JSONParserConfiguration.java\\\"\\n      ],\\n      \\\"dependencies\\\": [],\\n      \\\"api_endpoints\\\": [\\n        \\\"/json/parse\\\",\\n        \\\"/json/stringify\\\",\\n        \\\"/json/validate\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"MLService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Provides specialized JSONML (JSON Markup Language) functionalities.\\\",\\n        \\\"Handles conversion between XML and JSONML.\\\",\\n        \\\"Manages parsing configurations for XML and JSONML\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"JSONML.java\\\",\\n        \\\"JSONMLParserConfiguration.java\\\",\\n        \\\"XMLParserConfiguration.java\\\"\\n      ],\\n      \\\"dependencies\\\": [\\\"CoreService\\\"],\\n      \\\"api_endpoints\\\": [\\n        \\\"/jsonml/fromxml\\\",\\n        \\\"/jsonml/toxml\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"PointerService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Implements JSON Pointer functionality according to RFC 6901.\\\",\\n        \\\"Allows accessing values within JSON documents using pointer syntax.\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"JSONPointer.java\\\",\\n        \\\"JSONPointerException.java\\\"\\n      ],\\n      \\\"dependencies\\\": [\\\"CoreService\\\"],\\n      \\\"api_endpoints\\\": [\\n        \\\"/pointer/get\\\",\\n        \\\"/pointer/set\\\",\\n        \\\"/pointer/remove\\\"\\n      ]\\n    }\\n  ]\\n}\\n```\"\n",
            "          }\n",
            "        ],\n",
            "        \"role\": \"model\"\n",
            "      },\n",
            "      \"finishReason\": \"STOP\",\n",
            "      \"avgLogprobs\": -0.12847593214627956\n",
            "    }\n",
            "  ],\n",
            "  \"usageMetadata\": {\n",
            "    \"promptTokenCount\": 9214,\n",
            "    \"candidatesTokenCount\": 534,\n",
            "    \"totalTokenCount\": 9748,\n",
            "    \"promptTokensDetails\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"tokenCount\": 9214\n",
            "      }\n",
            "    ],\n",
            "    \"candidatesTokensDetails\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"tokenCount\": 534\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"modelVersion\": \"gemini-2.0-flash\",\n",
            "  \"responseId\": \"-3uqaMe4C8-x1MkP9vPK0QM\"\n",
            "}\n",
            "✅ Microservices plan generated at ./microservices/microservices_plan.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Use Colab's userdata to securely access the API key\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Endpoint for Gemini 2.0 Flash\n",
        "BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "\n",
        "def generate_gemini(prompt_text):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-goog-api-key\": API_KEY\n",
        "    }\n",
        "\n",
        "    # Prepare payload in the format Gemini expects\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\n",
        "                        \"text\": prompt_text\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(BASE_URL, headers=headers, json=data)\n",
        "    if response.status_code == 200:\n",
        "        resp_json = response.json()\n",
        "        # Print the full response for debugging\n",
        "        print(\"Full API response:\", json.dumps(resp_json, indent=2))\n",
        "        # Check for the expected nested structure\n",
        "        if \"candidates\" in resp_json and len(resp_json[\"candidates\"]) > 0 and \\\n",
        "           \"content\" in resp_json[\"candidates\"][0] and \\\n",
        "           \"parts\" in resp_json[\"candidates\"][0][\"content\"] and \\\n",
        "           len(resp_json[\"candidates\"][0][\"content\"][\"parts\"]) > 0 and \\\n",
        "           \"text\" in resp_json[\"candidates\"][0][\"content\"][\"parts\"][0]:\n",
        "            return resp_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise RuntimeError(f\"API response does not contain expected structure: {resp_json}\")\n",
        "    else:\n",
        "        raise RuntimeError(f\"API Error {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "def load_analysis_files(folder=\"analysis_output\", max_chars=5000):\n",
        "    \"\"\"\n",
        "    Read all analysis_output files (json, md, csv, etc.)\n",
        "    and concatenate into a compact string for analysis.\n",
        "    \"\"\"\n",
        "    contents = []\n",
        "    for fname in os.listdir(folder):\n",
        "        path = os.path.join(folder, fname)\n",
        "        if os.path.isfile(path):\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read()\n",
        "                    # truncate long files\n",
        "                    if len(text) > max_chars:\n",
        "                        text = text[:max_chars] + \"\\n...[TRUNCATED]...\"\n",
        "                    contents.append(f\"## {fname}\\n{text}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {fname}: {e}\")\n",
        "    return \"\\n\".join(contents)\n",
        "\n",
        "def generate_microservices_from_analysis(analysis_text, output_dir=\"./microservices\"):\n",
        "    \"\"\"\n",
        "    Use LLaMA AI agent to propose microservice decomposition\n",
        "    based on analysis_output scripts.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert software architect.\n",
        "    Given the following static analysis reports of a monolithic codebase,\n",
        "    propose a microservices architecture.\n",
        "\n",
        "    Requirements:\n",
        "    - Identify each microservice and its core responsibilities\n",
        "    - List the main functions/classes that belong to it\n",
        "    - Specify inter-service communication (REST, events, queues, etc.)\n",
        "    - Maintain functional equivalence to the monolith\n",
        "    - Avoid redundancy and enforce consistency across services\n",
        "    - Return output in strict JSON format:\n",
        "    {{\n",
        "        \"microservices\": [\n",
        "            {{\n",
        "                \"name\": \"...\",\n",
        "                \"responsibilities\": [\"...\"],\n",
        "                \"functions\": [\"...\"],\n",
        "                \"dependencies\": [\"...\"],\n",
        "                \"api_endpoints\": [\"...\"]\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Analysis Files Content:\n",
        "    {analysis_text}\n",
        "    \"\"\"\n",
        "\n",
        "    # Query llama model\n",
        "    response = generate_gemini(prompt)\n",
        "\n",
        "    # Assuming the response is directly the text content\n",
        "    raw_output = response.strip()\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Remove the markdown code block\n",
        "        if raw_output.startswith(\"```json\"):\n",
        "            raw_output = raw_output[7:]\n",
        "        if raw_output.endswith(\"```\"):\n",
        "            raw_output = raw_output[:-3]\n",
        "        raw_output = raw_output.strip()\n",
        "\n",
        "        microservices_plan = json.loads(raw_output)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ Could not parse LLaMA output as JSON. Raw output:\")\n",
        "        print(raw_output)\n",
        "        return None\n",
        "\n",
        "    # Save microservices architecture\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    arch_file = f\"{output_dir}/microservices_plan.json\"\n",
        "    with open(arch_file, \"w\") as f:\n",
        "        json.dump(microservices_plan, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Microservices plan generated at {arch_file}\")\n",
        "    return microservices_plan\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    analysis_text = load_analysis_files(\"analysis_output\", max_chars=3000)\n",
        "    microservices_plan = generate_microservices_from_analysis(analysis_text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da32f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1897ad3b-80b7-499d-88f3-2e1832cc6080"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Use Colab's userdata to securely access the API key\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Gemini 2.0 Flash endpoint\n",
        "BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Gemini API wrapper\n",
        "# ---------------------------\n",
        "def generate_gemini(prompt_text):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-goog-api-key\": API_KEY\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\"parts\": [{\"text\": prompt_text}]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(BASE_URL, headers=headers, json=data)\n",
        "    if response.status_code == 200:\n",
        "        resp_json = response.json()\n",
        "        if \"candidates\" in resp_json and \\\n",
        "           len(resp_json[\"candidates\"]) > 0 and \\\n",
        "           \"content\" in resp_json[\"candidates\"][0] and \\\n",
        "           \"parts\" in resp_json[\"candidates\"][0][\"content\"] and \\\n",
        "           len(resp_json[\"candidates\"][0][\"content\"][\"parts\"]) > 0 and \\\n",
        "           \"text\" in resp_json[\"candidates\"][0][\"content\"][\"parts\"][0]:\n",
        "            return resp_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise RuntimeError(f\"API response does not contain expected structure: {resp_json}\")\n",
        "    else:\n",
        "        raise RuntimeError(f\"API Error {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Load analysis files\n",
        "# ---------------------------\n",
        "def load_analysis_files(folder=\"analysis_output\", max_chars=5000):\n",
        "    contents = []\n",
        "    for fname in os.listdir(folder):\n",
        "        path = os.path.join(folder, fname)\n",
        "        if os.path.isfile(path):\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read()\n",
        "                    if len(text) > max_chars:\n",
        "                        text = text[:max_chars] + \"\\n...[TRUNCATED]...\"\n",
        "                    contents.append(f\"## {fname}\\n{text}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {fname}: {e}\")\n",
        "    return \"\\n\".join(contents)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Generate plan\n",
        "# ---------------------------\n",
        "def generate_microservices_from_analysis(analysis_text, output_dir=\"./microservices\"):\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert software architect.\n",
        "    Given the following static analysis reports of a monolithic codebase,\n",
        "    propose a microservices architecture.\n",
        "\n",
        "    Requirements:\n",
        "    - Identify each microservice and its core responsibilities\n",
        "    - List the main functions/classes that belong to it\n",
        "    - Specify inter-service communication (REST, events, queues, etc.)\n",
        "    - Maintain functional equivalence to the monolith\n",
        "    - Avoid redundancy and enforce consistency across services\n",
        "    - Return output in strict JSON format:\n",
        "    {{\n",
        "        \"microservices\": [\n",
        "            {{\n",
        "                \"name\": \"...\",\n",
        "                \"responsibilities\": [\"...\"],\n",
        "                \"functions\": [\"...\"],\n",
        "                \"dependencies\": [\"...\"],\n",
        "                \"api_endpoints\": [\"...\"]\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Analysis Files Content:\n",
        "    {analysis_text}\n",
        "    \"\"\"\n",
        "\n",
        "    raw_output = generate_gemini(prompt).strip()\n",
        "\n",
        "    try:\n",
        "        if raw_output.startswith(\"```json\"):\n",
        "            raw_output = raw_output[7:]\n",
        "        if raw_output.endswith(\"```\"):\n",
        "            raw_output = raw_output[:-3]\n",
        "        microservices_plan = json.loads(raw_output)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ Could not parse Gemini output as JSON. Raw output:\")\n",
        "        print(raw_output)\n",
        "        return None\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    arch_file = f\"{output_dir}/microservices_plan.json\"\n",
        "    with open(arch_file, \"w\") as f:\n",
        "        json.dump(microservices_plan, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Microservices plan generated at {arch_file}\")\n",
        "    return microservices_plan\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Generate code per service\n",
        "# ---------------------------\n",
        "def generate_microservice_code(service, output_dir=\"./microservices_code\"):\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert backend engineer.\n",
        "    Based on the following service definition, generate a scaffolded codebase\n",
        "    using Python FastAPI with REST endpoints.\n",
        "\n",
        "    Service definition:\n",
        "    {json.dumps(service, indent=2)}\n",
        "\n",
        "    Requirements:\n",
        "    - Create a main app.py with routes for each api_endpoint\n",
        "    - Include placeholder functions for responsibilities\n",
        "    - Add requirements.txt listing necessary libraries\n",
        "    - Organize into a modular folder structure\n",
        "    - Ensure the code runs with `uvicorn app:app --reload`\n",
        "    - most crucial is to Focus on the code corectness and workness with all of them are runnning correctly without any errors\n",
        "    - Return files with this format (IMPORTANT):\n",
        "      # file: path/to/file.py\n",
        "      ```python\n",
        "      # code here\n",
        "      ```\n",
        "    \"\"\"\n",
        "\n",
        "    response = generate_gemini(prompt)\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def save_microservice_code(service_name, code_text, base_dir=\"./microservices_code\"):\n",
        "    service_dir = os.path.join(base_dir, service_name)\n",
        "    os.makedirs(service_dir, exist_ok=True)\n",
        "\n",
        "    current_file = None\n",
        "    buffers = {}\n",
        "\n",
        "    for line in code_text.splitlines():\n",
        "        clean = line.strip()\n",
        "\n",
        "        if clean.lower().startswith(\"# file:\"):\n",
        "            current_file = clean.split(\":\", 1)[-1].strip()\n",
        "            buffers[current_file] = []\n",
        "        elif clean.startswith(\"```\"):\n",
        "            continue\n",
        "        elif current_file:\n",
        "            buffers[current_file].append(line)\n",
        "\n",
        "    if not buffers:\n",
        "        buffers[\"app.py\"] = [\n",
        "            l for l in code_text.splitlines() if not l.strip().startswith(\"```\")\n",
        "        ]\n",
        "\n",
        "    for fname, lines in buffers.items():\n",
        "        path = os.path.join(service_dir, fname)\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(lines).strip() + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Saved code for {service_name} in {service_dir}\")\n",
        "\n",
        "\n",
        "def generate_code_from_plan(plan_file=\"microservices/microservices_plan.json\"):\n",
        "    with open(plan_file, \"r\") as f:\n",
        "        plan = json.load(f)\n",
        "\n",
        "    for service in plan.get(\"microservices\", []):\n",
        "        name = service[\"name\"]\n",
        "        print(f\"🚀 Generating code for service: {name}\")\n",
        "        code_text = generate_microservice_code(service)\n",
        "        if code_text:\n",
        "            save_microservice_code(name, code_text)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Run end-to-end\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    analysis_text = load_analysis_files(\"analysis_output\", max_chars=3000)\n",
        "    microservices_plan = generate_microservices_from_analysis(analysis_text)\n",
        "\n",
        "    if microservices_plan:\n",
        "        print(\"🚀 Now generating microservice code...\")\n",
        "        generate_code_from_plan(\"microservices/microservices_plan.json\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Microservices plan generated at ./microservices/microservices_plan.json\n",
            "🚀 Now generating microservice code...\n",
            "🚀 Generating code for service: JSONService\n",
            "✅ Saved code for JSONService in ./microservices_code/JSONService\n",
            "🚀 Generating code for service: ParserConfigurationService\n",
            "✅ Saved code for ParserConfigurationService in ./microservices_code/ParserConfigurationService\n",
            "🚀 Generating code for service: UtilityService\n",
            "✅ Saved code for UtilityService in ./microservices_code/UtilityService\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = \"./microservices_code\"\n",
        "\n",
        "for service in os.listdir(BASE_DIR):\n",
        "    service_dir = os.path.join(BASE_DIR, service)\n",
        "    for root, _, files in os.walk(service_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".py\"):\n",
        "                path = os.path.join(root, file)\n",
        "                with open(path, \"r\") as f:\n",
        "                    lines = f.readlines()\n",
        "                # Remove lines with Markdown-style bold or numbered lists\n",
        "                clean_lines = [l for l in lines if not (\"**\" in l or l.lstrip().startswith(\"1.\"))]\n",
        "                # Add missing typing import if Optional used\n",
        "                text = \"\".join(clean_lines)\n",
        "                added_import = False\n",
        "                if \"Optional\" in text and \"from typing import Optional\" not in text:\n",
        "                    text = \"from typing import Optional\\n\" + text\n",
        "                    added_import = True\n",
        "                with open(path, \"w\") as f:\n",
        "                    f.write(text)\n",
        "                # Print what was fixed\n",
        "                print(f\"✅ Cleaned {service}/{os.path.relpath(path, service_dir)}\",\n",
        "                      \"(added typing import)\" if added_import else \"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vXfye6RALLiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"31ha53CJkpapgpKDuZvgEHJZpUR_42J9JRuciPggcPrUNPdLE\"\n",
        "\n"
      ],
      "metadata": {
        "id": "YNhbZ8KAfRUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok pydantic --quiet\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "BASE_DIR = \"./microservices_code\"\n",
        "PORT_START = 8000\n",
        "\n",
        "# ✅ Step 0: Configure ngrok authtoken\n",
        "# Make sure you've set NGROK_AUTH_TOKEN in your environment before running this\n",
        "auth_token = os.environ.get(\"NGROK_AUTH_TOKEN\")\n",
        "if not auth_token:\n",
        "    raise ValueError(\"❌ Please set NGROK_AUTH_TOKEN environment variable first.\")\n",
        "ngrok.set_auth_token(auth_token)   # ✅ Added line\n",
        "\n",
        "# Step 1: Clean generated code and fix imports\n",
        "for service in os.listdir(BASE_DIR):\n",
        "    service_dir = os.path.join(BASE_DIR, service)\n",
        "    for root, _, files in os.walk(service_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".py\"):\n",
        "                path = os.path.join(root, file)\n",
        "                with open(path, \"r\") as f:\n",
        "                    lines = f.readlines()\n",
        "                # Remove Markdown-like lines\n",
        "                clean_lines = [l for l in lines if not (\"**\" in l or l.lstrip().startswith(\"1.\"))]\n",
        "                text = \"\".join(clean_lines)\n",
        "                # Fix typing imports\n",
        "                added_import = False\n",
        "                if \"Optional\" in text or \"List\" in text or \"Dict\" in text:\n",
        "                    imports = []\n",
        "                    if \"Optional\" in text:\n",
        "                        imports.append(\"Optional\")\n",
        "                    if \"List\" in text:\n",
        "                        imports.append(\"List\")\n",
        "                    if \"Dict\" in text:\n",
        "                        imports.append(\"Dict\")\n",
        "                    import_line = f\"from typing import {', '.join(imports)}\\n\"\n",
        "                    text = import_line + text\n",
        "                    added_import = True\n",
        "                # Ensure FastAPI and Pydantic imports\n",
        "                if \"FastAPI\" in text and \"from fastapi import FastAPI\" not in text:\n",
        "                    text = \"from fastapi import FastAPI, APIRouter\\n\" + text\n",
        "                if \"BaseModel\" in text and \"from pydantic import BaseModel\" not in text:\n",
        "                    text = \"from pydantic import BaseModel\\n\" + text\n",
        "                with open(path, \"w\") as f:\n",
        "                    f.write(text)\n",
        "                print(f\"✅ Cleaned {service}/{os.path.relpath(path, service_dir)}\",\n",
        "                      \"(added typing import)\" if added_import else \"\")\n",
        "\n",
        "# Step 2: Kill previous uvicorn processes if any (avoid port in use)\n",
        "for port in range(PORT_START, PORT_START+10):\n",
        "    try:\n",
        "        subprocess.run(f\"fuser -k {port}/tcp\", shell=True, check=True)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Step 3: Start services and expose via ngrok\n",
        "ngrok_tunnels = {}\n",
        "for i, service in enumerate(sorted(os.listdir(BASE_DIR))):\n",
        "    service_dir = os.path.join(BASE_DIR, service)\n",
        "    port = PORT_START + i\n",
        "    print(f\"🚀 Starting {service} on port {port}...\")\n",
        "    # Start uvicorn in background\n",
        "    subprocess.Popen(\n",
        "        [\"python\", \"-m\", \"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", str(port)],\n",
        "        cwd=service_dir\n",
        "    )\n",
        "    # ✅ Added: specify auth token is already set\n",
        "    public_url = ngrok.connect(port, \"http\")\n",
        "    ngrok_tunnels[service] = public_url\n",
        "\n",
        "# Step 4: Wait a few seconds to ensure services are up\n",
        "time.sleep(5)\n",
        "\n",
        "# Step 5: Print all public URLs\n",
        "print(\"\\n✅ Microservices are live at:\")\n",
        "for service, url in ngrok_tunnels.items():\n",
        "    print(f\"{service}: {url}\")\n"
      ],
      "metadata": {
        "id": "bsFHa2EhS_av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files # Explicitly import files for download\n",
        "\n",
        "shutil.make_archive(\"microservices_code\", 'zip', \"/content/microservices_code\")\n",
        "files.download(\"microservices_code.zip\")"
      ],
      "metadata": {
        "id": "bC_lB_TOU4ov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e9e25735-e9ce-4e60-a762-75fbd26278ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9ab1cc2b-1c7c-4f2d-9a1d-1b32bc51fc41\", \"microservices_code.zip\", 9303)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iwXFZp_NU7-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}