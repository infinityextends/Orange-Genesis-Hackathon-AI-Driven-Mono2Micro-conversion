{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Have defined functions to access the data from a Monolithic Data Source.\n",
        "\n",
        "The functions defined are for :\n",
        "Cloning the GitHub Repository\n",
        "Tranversing through all the files to easy out the sorting method.\n",
        "Give a JSON file."
      ],
      "metadata": {
        "id": "ECaeYff0L64Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqXm1x-SJvNL",
        "outputId": "a34a0b87-dca6-4931-8924-ac266fcc0464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting monolith_inspector.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile monolith_inspector.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Monolith Inspector for Java (jPetStore-ready)\n",
        "\n",
        "What it does\n",
        "------------\n",
        "- Clones a repo (optional) OR scans a local path\n",
        "- Emits:\n",
        "  out/\n",
        "    file_structure.md\n",
        "    file_structure.json\n",
        "    knowledge_graph.graphml\n",
        "    knowledge_graph.json\n",
        "    coupling_metrics.csv   # package-level afferent/efferent coupling\n",
        "\n",
        "Graph nodes\n",
        "-----------\n",
        "- package: com.example.foo\n",
        "- class:   com.example.foo.Bar\n",
        "- method:  com.example.foo.Bar#baz(argTypes)\n",
        "\n",
        "Graph edges (MultiDiGraph, directed)\n",
        "------------------------------------\n",
        "- package -> class                  (contains)\n",
        "- class   -> class                  (extends / implements)\n",
        "- class   -> class/package          (import depends-on)\n",
        "- class   -> method                 (defines)\n",
        "- class   -> class/method?          (calls – best-effort via javalang)\n",
        "\n",
        "Notes\n",
        "-----\n",
        "- Java resolution of method calls is heuristic (no type solver). Still great for macro structure.\n",
        "- Safe on large repos; skips generated/build dirs.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import javalang\n",
        "import networkx as nx\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "\n",
        "SKIP_DIRS = {\n",
        "    \".git\", \".idea\", \".vscode\", \"target\", \"build\", \"out\", \".gradle\", \".mvn\",\n",
        "    \"node_modules\"\n",
        "}\n",
        "\n",
        "JAVA_EXT = {\".java\"}\n",
        "\n",
        "def run(cmd, cwd=None) -> str:\n",
        "    p = subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed: {' '.join(cmd)}\\n{p.stdout}\")\n",
        "    return p.stdout\n",
        "\n",
        "def maybe_clone(repo, branch, dest) -> Path:\n",
        "    dest = Path(dest).expanduser().resolve()\n",
        "    if dest.exists() and any(dest.iterdir()):\n",
        "        return dest\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    run([\"git\", \"clone\", \"--depth\", \"1\", \"-b\", branch, repo, str(dest)])\n",
        "    return dest\n",
        "\n",
        "def walk_files(root: Path):\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        # prune skip dirs in-place\n",
        "        dirnames[:] = [d for d in dirnames if d not in SKIP_DIRS and not d.startswith(\".\")]\n",
        "        for f in filenames:\n",
        "            p = Path(dirpath) / f\n",
        "            if p.suffix.lower() in JAVA_EXT:\n",
        "                yield p\n",
        "\n",
        "def build_tree_markdown(root: Path) -> str:\n",
        "    \"\"\"Return a Markdown tree (like tree) but light-weight.\"\"\"\n",
        "    lines = [f\"# File Structure for {root.name}\\n\"]\n",
        "\n",
        "    def relpath(p): return str(p.relative_to(root))\n",
        "\n",
        "    def tree(prefix: str, path: Path):\n",
        "        entries = sorted([*path.iterdir()], key=lambda x: (x.is_file(), x.name.lower()))\n",
        "        for i, e in enumerate(entries):\n",
        "            if e.name in SKIP_DIRS or e.name.startswith(\".\"):\n",
        "                continue\n",
        "            connector = \"└─\" if i == len(entries)-1 else \"├─\"\n",
        "            lines.append(f\"{prefix}{connector} {e.name}\")\n",
        "            if e.is_dir():\n",
        "                child_prefix = f\"{prefix}{'   ' if i == len(entries)-1 else '│  '}\"\n",
        "                tree(child_prefix, e)\n",
        "\n",
        "    tree(\"\", root)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def tree_to_json(root: Path):\n",
        "    def node(path: Path):\n",
        "        if path.is_dir():\n",
        "            return {\n",
        "                \"name\": path.name,\n",
        "                \"type\": \"dir\",\n",
        "                \"children\": [\n",
        "                    node(c) for c in sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))\n",
        "                    if c.name not in SKIP_DIRS and not c.name.startswith(\".\")\n",
        "                ]\n",
        "            }\n",
        "        else:\n",
        "            return {\"name\": path.name, \"type\": \"file\"}\n",
        "    return node(root)\n",
        "\n",
        "# ----------------------------\n",
        "# Java parsing (javalang)\n",
        "# ----------------------------\n",
        "\n",
        "def parse_java(path: Path):\n",
        "    try:\n",
        "        src = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        tree = javalang.parse.parse(src)\n",
        "        return src, tree\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def locate_package(tree) -> str:\n",
        "    try:\n",
        "        return tree.package.name if tree.package else \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# from typing import List\n",
        "def short_to_fqcn(short: str, pkg: str, imports: List[str]) -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Best effort: if short is fully qualified, keep it.\n",
        "    If not, try imports; else qualify with current package.\n",
        "    \"\"\"\n",
        "    if \".\" in short and short[0].islower() is False:  # looks like fqcn (e.g., com.foo.Bar)\n",
        "        return short\n",
        "    base = short.split(\".\")[0]\n",
        "    for imp in imports:\n",
        "        if imp.endswith(\".\" + base) or imp.endswith(\".\" + base + \".*\"):\n",
        "            # if wildcard import, we can't pin class – return package-level\n",
        "            if imp.endswith(\".*\"):\n",
        "                return imp[:-2] + \".\" + base\n",
        "            return imp\n",
        "    return f\"{pkg}.{base}\" if pkg else base\n",
        "\n",
        "def method_sig(pkg: str, cls: str, m) -> str:\n",
        "    # com.pets.Store#find(String,int)\n",
        "    params = \",\".join([getattr(p.type, \"name\", \"Object\") for p in (m.parameters or [])])\n",
        "    return f\"{pkg}.{cls}#{m.name}({params})\" if pkg else f\"{cls}#{m.name}({params})\"\n",
        "\n",
        "# ----------------------------\n",
        "# Graph construction\n",
        "# ----------------------------\n",
        "\n",
        "def analyze_repo(root: Path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - graph (MultiDiGraph)\n",
        "      - file_index: list of java files processed\n",
        "      - package_coupling: dict of pkg -> {\"afferent\": set, \"efferent\": set}\n",
        "    \"\"\"\n",
        "    G = nx.MultiDiGraph()\n",
        "    file_index = []\n",
        "    package_coupling = defaultdict(lambda: {\"afferent\": set(), \"efferent\": set()})\n",
        "\n",
        "    for jf in walk_files(root):\n",
        "        src, tree = parse_java(jf)\n",
        "        if tree is None:\n",
        "            continue\n",
        "        file_index.append(str(jf.relative_to(root)))\n",
        "\n",
        "        pkg = locate_package(tree)\n",
        "        imports = [imp.path for imp in (tree.imports or []) if hasattr(imp, \"path\")]\n",
        "        # add package node\n",
        "        if pkg:\n",
        "            G.add_node(f\"package:{pkg}\", kind=\"package\", name=pkg)\n",
        "\n",
        "        # collect class/interface declarations\n",
        "        types = [t for t in tree.types if hasattr(t, \"name\")]\n",
        "        for t in types:\n",
        "            cls_name = t.name\n",
        "            fqcn = f\"{pkg}.{cls_name}\" if pkg else cls_name\n",
        "            G.add_node(f\"class:{fqcn}\", kind=\"class\", name=fqcn, file=str(jf.relative_to(root)))\n",
        "\n",
        "            # package contains class\n",
        "            if pkg:\n",
        "                G.add_edge(f\"package:{pkg}\", f\"class:{fqcn}\", kind=\"contains\")\n",
        "\n",
        "            # extends/implements edges\n",
        "            if getattr(t, \"extends\", None):\n",
        "                base = getattr(t.extends, \"name\", None) or str(t.extends)\n",
        "                super_fq = short_to_fqcn(base, pkg, imports)\n",
        "                G.add_node(f\"class:{super_fq}\", kind=\"class\", name=super_fq)\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"class:{super_fq}\", kind=\"extends\")\n",
        "\n",
        "                # coupling: class pkg depends on super pkg\n",
        "                super_pkg = \".\".join(super_fq.split(\".\")[:-1])\n",
        "                if pkg and super_pkg and super_pkg != pkg:\n",
        "                    package_coupling[pkg][\"efferent\"].add(super_pkg)\n",
        "                    package_coupling[super_pkg][\"afferent\"].add(pkg)\n",
        "\n",
        "            impls = getattr(t, \"implements\", []) or []\n",
        "            for impl in impls:\n",
        "                iname = getattr(impl, \"name\", None) or str(impl)\n",
        "                int_fq = short_to_fqcn(iname, pkg, imports)\n",
        "                G.add_node(f\"class:{int_fq}\", kind=\"class\", name=int_fq)\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"class:{int_fq}\", kind=\"implements\")\n",
        "\n",
        "                int_pkg = \".\".join(int_fq.split(\".\")[:-1])\n",
        "                if pkg and int_pkg and int_pkg != pkg:\n",
        "                    package_coupling[pkg][\"efferent\"].add(int_pkg)\n",
        "                    package_coupling[int_pkg][\"afferent\"].add(pkg)\n",
        "\n",
        "            # import edges (package-level dependency)\n",
        "            for imp in imports:\n",
        "                # if import is a class, link class->that class; also package coupling\n",
        "                imported = imp\n",
        "                G.add_node(f\"class:{imported}\", kind=\"class\", name=imported)\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"class:{imported}\", kind=\"imports\")\n",
        "\n",
        "                imp_pkg = \".\".join(imported.split(\".\")[:-1]) if \".\" in imported else \"\"\n",
        "                if pkg and imp_pkg and imp_pkg != pkg and not imported.endswith(\".*\"):\n",
        "                    package_coupling[pkg][\"efferent\"].add(imp_pkg)\n",
        "                    package_coupling[imp_pkg][\"afferent\"].add(pkg)\n",
        "\n",
        "            # methods + definitions\n",
        "            methods = [m for m in getattr(t, \"methods\", [])]\n",
        "            for m in methods:\n",
        "                msig = method_sig(pkg, cls_name, m)\n",
        "                G.add_node(f\"method:{msig}\", kind=\"method\", name=msig, file=str(jf.relative_to(root)))\n",
        "                G.add_edge(f\"class:{fqcn}\", f\"method:{msig}\", kind=\"defines\")\n",
        "\n",
        "                # method calls (best-effort)\n",
        "                # javalang gives MethodInvocation with .qualifier and .member\n",
        "                # We’ll edge from class -> (guessed) target class or method token\n",
        "                body = m.body or []\n",
        "                try:\n",
        "                    # flatten nodes\n",
        "                    for path, node in m.filter(javalang.tree.MethodInvocation):\n",
        "                        qual = node.qualifier  # may be a class/variable name\n",
        "                        member = node.member\n",
        "                        target = None\n",
        "\n",
        "                        if qual:\n",
        "                            target = short_to_fqcn(qual, pkg, imports)\n",
        "                            G.add_node(f\"class:{target}\", kind=\"class\", name=target)\n",
        "                            G.add_edge(f\"class:{fqcn}\", f\"class:{target}\", kind=\"calls\")\n",
        "\n",
        "                            # package coupling for calls\n",
        "                            tgt_pkg = \".\".join(target.split(\".\")[:-1])\n",
        "                            if pkg and tgt_pkg and tgt_pkg != pkg:\n",
        "                                package_coupling[pkg][\"efferent\"].add(tgt_pkg)\n",
        "                                package_coupling[tgt_pkg][\"afferent\"].add(pkg)\n",
        "                        else:\n",
        "                            # unknown receiver – record as method token under this class namespace\n",
        "                            token = f\"{pkg}.{member}\" if pkg else member\n",
        "                            G.add_node(f\"method:{token}\", kind=\"method\", name=token)\n",
        "                            G.add_edge(f\"class:{fqcn}\", f\"method:{token}\", kind=\"calls\")\n",
        "\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    return G, file_index, package_coupling\n",
        "\n",
        "# ----------------------------\n",
        "# Outputs\n",
        "# ----------------------------\n",
        "\n",
        "def write_outputs(root: Path, outdir: Path, G: nx.MultiDiGraph, file_index, package_coupling):\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # file structure\n",
        "    md = build_tree_markdown(root)\n",
        "    (outdir / \"file_structure.md\").write_text(md, encoding=\"utf-8\")\n",
        "    struct_json = tree_to_json(root)\n",
        "    (outdir / \"file_structure.json\").write_text(json.dumps(struct_json, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # graph\n",
        "    nx.write_graphml(G, outdir / \"knowledge_graph.graphml\")\n",
        "\n",
        "    # json dump\n",
        "    nodes = [{\"id\": n, **G.nodes[n]} for n in G.nodes]\n",
        "    edges = [{\"u\": u, \"v\": v, \"key\": k, **G.edges[u, v, k]} for u, v, k in G.edges(keys=True)]\n",
        "    (outdir / \"knowledge_graph.json\").write_text(json.dumps({\"nodes\": nodes, \"edges\": edges}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # coupling metrics\n",
        "    import csv\n",
        "    with (outdir / \"coupling_metrics.csv\").open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"package\", \"afferent_count\", \"efferent_count\", \"afferent_set\", \"efferent_set\"])\n",
        "        for pkg, ce in sorted(package_coupling.items()):\n",
        "            aff = sorted(ce[\"afferent\"])\n",
        "            eff = sorted(ce[\"efferent\"])\n",
        "            w.writerow([pkg, len(aff), len(eff), \";\".join(aff), \";\".join(eff)])\n",
        "\n",
        "    # tiny console summary\n",
        "    print(f\"[OK] Wrote outputs to: {outdir}\")\n",
        "    print(f\" - file_structure.md / .json\")\n",
        "    print(f\" - knowledge_graph.graphml / .json\")\n",
        "    print(f\" - coupling_metrics.csv\")\n",
        "    print(f\"Java files parsed: {len(file_index)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# CLI\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Parse Java monolith (like jPetStore) and build a knowledge graph.\")\n",
        "    ap.add_argument(\"--repo\", help=\"Git repo URL (optional). If omitted, use --path.\", default=None)\n",
        "    ap.add_argument(\"--branch\", help=\"Branch to clone\", default=\"master\")\n",
        "    ap.add_argument(\"--path\", help=\"Local path to repo (if you already cloned).\", default=None)\n",
        "    ap.add_argument(\"--out\", help=\"Output directory\", default=\"out\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    if not args.repo and not args.path:\n",
        "        ap.error(\"Provide either --repo or --path\")\n",
        "\n",
        "    if args.repo:\n",
        "        root = maybe_clone(args.repo, args.branch, \"./_repo_checkout\")\n",
        "    else:\n",
        "        root = Path(args.path).expanduser().resolve()\n",
        "        if not root.exists():\n",
        "            raise SystemExit(f\"Path not found: {root}\")\n",
        "\n",
        "    G, files, coupling = analyze_repo(root)\n",
        "    write_outputs(root, Path(args.out), G, files, coupling)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f12031d",
        "outputId": "f601f37c-6cd3-46f9-ce35-754bb0f70fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: javalang in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from javalang) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install javalang networkx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes the Monolithic Data from an open source GitHub link, and runs the functions defined previously."
      ],
      "metadata": {
        "id": "LzLRQV8TM-qU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a3e9df6",
        "outputId": "1d8769fb-be51-4839-98bd-62b34f5d964f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script executed successfully!\n",
            "Stdout:\n",
            " [OK] Wrote outputs to: analysis_output\n",
            " - file_structure.md / .json\n",
            " - knowledge_graph.graphml / .json\n",
            " - coupling_metrics.csv\n",
            "Java files parsed: 72\n",
            "\n",
            "Stderr:\n",
            " \n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Create a directory for the output\n",
        "output_dir = \"analysis_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Run the script with a sample repo (jPetStore)\n",
        "# Assuming the script is saved as 'monolith_inspector.py' in the current directory\n",
        "script_name = \"monolith_inspector.py\"\n",
        "\n",
        "# The script content is already in monolith_inspector.py due to the %%writefile magic command\n",
        "\n",
        "command = [\"python3\", script_name, \"--repo\", \"https://github.com/KimJongSung/jPetStore.git\", \"--out\", output_dir]\n",
        "\n",
        "try:\n",
        "    process = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "    print(\"Script executed successfully!\")\n",
        "    print(\"Stdout:\\n\", process.stdout)\n",
        "    print(\"Stderr:\\n\", process.stderr)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error executing script: {e}\")\n",
        "    print(\"Stdout:\\n\", e.stdout)\n",
        "    print(\"Stderr:\\n\", e.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying a sample size of the knowledge graph created."
      ],
      "metadata": {
        "id": "Mx94yW3POAL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8e75ebc",
        "outputId": "af16c42d-80b2-4eda-b191-4f0b2887e5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Graph JSON structure:\n",
            "Keys: dict_keys(['nodes', 'edges'])\n",
            "\n",
            "First 5 Nodes:\n",
            "{'id': 'package:org.springframework.samples.jpetstore.domain', 'kind': 'package', 'name': 'org.springframework.samples.jpetstore.domain'}\n",
            "{'id': 'class:org.springframework.samples.jpetstore.domain.Product', 'kind': 'class', 'name': 'org.springframework.samples.jpetstore.domain.Product', 'file': 'src/main/java/org/springframework/samples/jpetstore/domain/Product.java'}\n",
            "{'id': 'class:java.io.Serializable', 'kind': 'class', 'name': 'java.io.Serializable'}\n",
            "{'id': 'method:org.springframework.samples.jpetstore.domain.Product#getProductId()', 'kind': 'method', 'name': 'org.springframework.samples.jpetstore.domain.Product#getProductId()', 'file': 'src/main/java/org/springframework/samples/jpetstore/domain/Product.java'}\n",
            "{'id': 'method:org.springframework.samples.jpetstore.domain.Product#setProductId(String)', 'kind': 'method', 'name': 'org.springframework.samples.jpetstore.domain.Product#setProductId(String)', 'file': 'src/main/java/org/springframework/samples/jpetstore/domain/Product.java'}\n",
            "\n",
            "First 5 Edges:\n",
            "{'u': 'package:org.springframework.samples.jpetstore.domain', 'v': 'class:org.springframework.samples.jpetstore.domain.Product', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.springframework.samples.jpetstore.domain', 'v': 'class:org.springframework.samples.jpetstore.domain.Category', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.springframework.samples.jpetstore.domain', 'v': 'class:org.springframework.samples.jpetstore.domain.LineItem', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.springframework.samples.jpetstore.domain', 'v': 'class:org.springframework.samples.jpetstore.domain.CartItem', 'key': 0, 'kind': 'contains'}\n",
            "{'u': 'package:org.springframework.samples.jpetstore.domain', 'v': 'class:org.springframework.samples.jpetstore.domain.Cart', 'key': 0, 'kind': 'contains'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "output_dir = \"analysis_output\"\n",
        "graph_json_path = os.path.join(output_dir, \"knowledge_graph.json\")\n",
        "\n",
        "if os.path.exists(graph_json_path):\n",
        "    with open(graph_json_path, 'r') as f:\n",
        "        graph_data = json.load(f)\n",
        "    # Displaying the full JSON might be too large, so display a part of it or a summary\n",
        "    # For simplicity, let's display the keys and the first few items of nodes and edges\n",
        "    print(\"Knowledge Graph JSON structure:\")\n",
        "    print(f\"Keys: {graph_data.keys()}\")\n",
        "    print(\"\\nFirst 5 Nodes:\")\n",
        "    for i, node in enumerate(graph_data.get('nodes', [])[:5]):\n",
        "        print(node)\n",
        "    print(\"\\nFirst 5 Edges:\")\n",
        "    for i, edge in enumerate(graph_data.get('edges', [])[:5]):\n",
        "        print(edge)\n",
        "\n",
        "else:\n",
        "    print(f\"Knowledge graph JSON file not found at: {graph_json_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to take the structural knowledge graph and convert it into a semantic context to enable accurate queries and estabilish showing how the code elements are connected.\n",
        "\n",
        "Knowledge Graph holds the nodes - entities with texts and fake embeddings, and edges which contains the relationships.\n",
        "\n",
        "We output the final enriched graph and index for nodes."
      ],
      "metadata": {
        "id": "SsaMFpMHOXCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTnSIa3YMO-",
        "outputId": "08872991-a2a0-4d76-aa18-b5cb606032d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting graphrag_merger.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile graphrag_merger.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GraphRAG Merger\n",
        "\n",
        "Takes the structural knowledge graph (from monolith_inspector.py)\n",
        "and enriches it with semantic/functional context for RAG-style queries.\n",
        "\n",
        "Inputs\n",
        "------\n",
        "- knowledge_graph.json (nodes + edges from networkx export)\n",
        "- file_structure.json  (hierarchical tree)\n",
        "- coupling_metrics.csv (afferent/efferent coupling)\n",
        "\n",
        "Outputs\n",
        "-------\n",
        "out/\n",
        "  graph_context.json   # GraphRAG-compatible schema (entities + relationships)\n",
        "  node_index.json      # index with text chunks + embeddings\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import hashlib\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "\n",
        "def read_json(path: Path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def read_csv(path: Path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for row in csv.DictReader(f):\n",
        "            rows.append(row)\n",
        "    return rows\n",
        "\n",
        "def simple_embed(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Placeholder: simple hash-based embedding\n",
        "    (replace with OpenAI, HuggingFace, etc. if needed).\n",
        "    \"\"\"\n",
        "    h = hashlib.sha256(text.encode(\"utf-8\")).digest()\n",
        "    return [x/255 for x in h[:32]]  # 32-dim fake embedding\n",
        "\n",
        "# ----------------------------\n",
        "# Build GraphRAG schema\n",
        "# ----------------------------\n",
        "\n",
        "def build_graphrag_context(graph_data: Dict[str, Any],\n",
        "                           file_struct: Dict[str, Any],\n",
        "                           coupling: List[Dict[str, str]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convert nodes + edges + file context into GraphRAG style:\n",
        "      {\n",
        "        \"entities\": [ {id, type, name, text, embedding} ],\n",
        "        \"relationships\": [ {source, target, type} ]\n",
        "      }\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    relationships = []\n",
        "\n",
        "    # Convert nodes\n",
        "    for n in graph_data[\"nodes\"]:\n",
        "        node_id = n[\"id\"]\n",
        "        node_type = n.get(\"kind\", \"unknown\")\n",
        "        name = n.get(\"name\", node_id)\n",
        "\n",
        "        text = f\"{node_type} {name}\"\n",
        "        if \"file\" in n:\n",
        "            text += f\" defined in {n['file']}\"\n",
        "\n",
        "        entities.append({\n",
        "            \"id\": node_id,\n",
        "            \"type\": node_type,\n",
        "            \"name\": name,\n",
        "            \"text\": text,\n",
        "            \"embedding\": simple_embed(text)\n",
        "        })\n",
        "\n",
        "    # Convert edges\n",
        "    for e in graph_data[\"edges\"]:\n",
        "        relationships.append({\n",
        "            \"source\": e[\"u\"],\n",
        "            \"target\": e[\"v\"],\n",
        "            \"type\": e.get(\"kind\", \"related\")\n",
        "        })\n",
        "\n",
        "    # Enrich with coupling as extra edges\n",
        "    for row in coupling:\n",
        "        pkg = row[\"package\"]\n",
        "        for eff in row[\"efferent_set\"].split(\";\"):\n",
        "            if eff:\n",
        "                relationships.append({\n",
        "                    \"source\": f\"package:{pkg}\",\n",
        "                    \"target\": f\"package:{eff}\",\n",
        "                    \"type\": \"couples-to\"\n",
        "                })\n",
        "\n",
        "    return {\"entities\": entities, \"relationships\": relationships}\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "\n",
        "def main(outdir=\"analysis_output\"):\n",
        "    out = Path(outdir)\n",
        "    graph_data = read_json(out / \"knowledge_graph.json\")\n",
        "    file_struct = read_json(out / \"file_structure.json\")\n",
        "    coupling = read_csv(out / \"coupling_metrics.csv\")\n",
        "\n",
        "    merged = build_graphrag_context(graph_data, file_struct, coupling)\n",
        "\n",
        "    (out / \"graph_context.json\").write_text(\n",
        "        json.dumps(merged, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    # Build node index for retrieval\n",
        "    node_index = {\n",
        "        e[\"id\"]: {\"text\": e[\"text\"], \"embedding\": e[\"embedding\"]}\n",
        "        for e in merged[\"entities\"]\n",
        "    }\n",
        "    (out / \"node_index.json\").write_text(\n",
        "        json.dumps(node_index, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "    print(f\"[OK] GraphRAG context written to {out/'graph_context.json'}\")\n",
        "    print(f\"[OK] Node index written to {out/'node_index.json'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbz6g7t7Yh-u",
        "outputId": "972364bc-29af-4040-f8ab-bcd9742ac106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] GraphRAG context written to analysis_output/graph_context.json\n",
            "[OK] Node index written to analysis_output/node_index.json\n"
          ]
        }
      ],
      "source": [
        "!python3 graphrag_merger.py --out out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up an API key for our code."
      ],
      "metadata": {
        "id": "6dKv7I75Phms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "import os\n",
        "\n",
        "# Replace with your actual API key\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyB3II7ESGsgzPcHZ440X4MLoHkJdPXirG0\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkYMImWl70yA",
        "outputId": "d8922b64-aac9-46df-af96-7f4b309528d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now We input the files from graphrag merger, consisting the analysis (e.g., graph_context.json, node_index.json), and merge them into one text string.\n",
        "\n",
        "Gemini API helps us propose a microservices architecture in strict JSON format.\n",
        "\n",
        "That consists of:\n",
        "\n",
        "Microservice names\n",
        "\n",
        "Responsibilities\n",
        "\n",
        "Functions/classes it handles\n",
        "\n",
        "Dependencies\n",
        "\n",
        "API endpoints"
      ],
      "metadata": {
        "id": "prLmzGdrQXJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paRKmuvaeeL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be80b639-578b-4863-c9ce-88c1d24a0591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full API response: {\n",
            "  \"candidates\": [\n",
            "    {\n",
            "      \"content\": {\n",
            "        \"parts\": [\n",
            "          {\n",
            "            \"text\": \"```json\\n{\\n  \\\"microservices\\\": [\\n    {\\n      \\\"name\\\": \\\"CatalogService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Manage product catalog, including categories, products, and items.\\\",\\n        \\\"Provide APIs to browse and search the catalog.\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"org.springframework.samples.jpetstore.domain.Product\\\",\\n        \\\"org.springframework.samples.jpetstore.domain.Category\\\",\\n        \\\"org.springframework.samples.jpetstore.domain.Item\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ProductDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.CategoryDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ItemDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ibatis.SqlMapProductDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ibatis.SqlMapCategoryDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ibatis.SqlMapItemDao\\\"\\n      ],\\n      \\\"dependencies\\\": [],\\n      \\\"api_endpoints\\\": [\\n        \\\"/catalog/categories\\\",\\n        \\\"/catalog/categories/{categoryId}\\\",\\n        \\\"/catalog/products/{productId}\\\",\\n        \\\"/catalog/items/{itemId}\\\",\\n        \\\"/catalog/products/{productId}/items\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"AccountService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Manage user accounts, including registration, login, and profile management.\\\",\\n        \\\"Authentication and authorization.\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"org.springframework.samples.jpetstore.domain.Account\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.AccountDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ibatis.SqlMapAccountDao\\\",\\n        \\\"org.springframework.samples.jpetstore.web.spring.AccountForm\\\",\\n        \\\"org.springframework.samples.jpetstore.web.spring.AccountFormController\\\"\\n      ],\\n      \\\"dependencies\\\": [],\\n      \\\"api_endpoints\\\": [\\n        \\\"/accounts/register\\\",\\n        \\\"/accounts/login\\\",\\n        \\\"/accounts/profile\\\",\\n        \\\"/accounts/{username}\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"OrderService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Manage orders, including creating, updating, and retrieving orders.\\\",\\n        \\\"Order validation.\\\",\\n        \\\"Inventory management (decreasing available quantity of items upon successful order).\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"org.springframework.samples.jpetstore.domain.Order\\\",\\n        \\\"org.springframework.samples.jpetstore.domain.LineItem\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.OrderDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ibatis.SqlMapOrderDao\\\",\\n        \\\"org.springframework.samples.jpetstore.domain.logic.OrderService\\\",\\n        \\\"org.springframework.samples.jpetstore.domain.logic.OrderValidator\\\",\\n        \\\"org.springframework.samples.jpetstore.domain.Cart\\\",\\n        \\\"org.springframework.samples.jpetstore.domain.CartItem\\\"\\n      ],\\n      \\\"dependencies\\\": [\\n        \\\"CatalogService\\\",\\n        \\\"AccountService\\\",\\n        \\\"InventoryService\\\"\\n      ],\\n      \\\"api_endpoints\\\": [\\n        \\\"/orders\\\",\\n        \\\"/orders/{orderId}\\\",\\n        \\\"/orders/{orderId}/lineItems\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"InventoryService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Manage inventory levels for each item.\\\",\\n        \\\"Provide APIs to check availability and update inventory.\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"org.springframework.samples.jpetstore.domain.Item\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ItemDao\\\",\\n        \\\"org.springframework.samples.jpetstore.dao.ibatis.SqlMapItemDao\\\"\\n      ],\\n      \\\"dependencies\\\": [\\n        \\\"CatalogService\\\"\\n      ],\\n      \\\"api_endpoints\\\": [\\n        \\\"/inventory/{itemId}\\\",\\n        \\\"/inventory/{itemId}/availability\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"NotificationService\\\",\\n      \\\"responsibilities\\\": [\\n        \\\"Send order confirmation emails and other notifications.\\\"\\n      ],\\n      \\\"functions\\\": [\\n        \\\"org.springframework.samples.jpetstore.domain.logic.SendOrderConfirmationEmailAdvice\\\"\\n      ],\\n      \\\"dependencies\\\": [\\n        \\\"AccountService\\\",\\n        \\\"OrderService\\\"\\n      ],\\n      \\\"api_endpoints\\\": []\\n    }\\n  ]\\n}\\n```\"\n",
            "          }\n",
            "        ],\n",
            "        \"role\": \"model\"\n",
            "      },\n",
            "      \"finishReason\": \"STOP\",\n",
            "      \"avgLogprobs\": -0.06322261894747429\n",
            "    }\n",
            "  ],\n",
            "  \"usageMetadata\": {\n",
            "    \"promptTokenCount\": 9031,\n",
            "    \"candidatesTokenCount\": 1039,\n",
            "    \"totalTokenCount\": 10070,\n",
            "    \"promptTokensDetails\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"tokenCount\": 9031\n",
            "      }\n",
            "    ],\n",
            "    \"candidatesTokensDetails\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"tokenCount\": 1039\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"modelVersion\": \"gemini-2.0-flash\",\n",
            "  \"responseId\": \"-OCpaOrmAsbVz7IPqMaamQI\"\n",
            "}\n",
            "✅ Microservices plan generated at ./microservices/microservices_plan.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Use Colab's userdata to securely access the API key\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Endpoint for Gemini 2.0 Flash\n",
        "BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "\n",
        "def generate_gemini(prompt_text):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-goog-api-key\": API_KEY\n",
        "    }\n",
        "\n",
        "    # Prepare payload in the format Gemini expects\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\n",
        "                        \"text\": prompt_text\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(BASE_URL, headers=headers, json=data)\n",
        "    if response.status_code == 200:\n",
        "        resp_json = response.json()\n",
        "        # Print the full response for debugging\n",
        "        print(\"Full API response:\", json.dumps(resp_json, indent=2))\n",
        "        # Check for the expected nested structure\n",
        "        if \"candidates\" in resp_json and len(resp_json[\"candidates\"]) > 0 and \\\n",
        "           \"content\" in resp_json[\"candidates\"][0] and \\\n",
        "           \"parts\" in resp_json[\"candidates\"][0][\"content\"] and \\\n",
        "           len(resp_json[\"candidates\"][0][\"content\"][\"parts\"]) > 0 and \\\n",
        "           \"text\" in resp_json[\"candidates\"][0][\"content\"][\"parts\"][0]:\n",
        "            return resp_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise RuntimeError(f\"API response does not contain expected structure: {resp_json}\")\n",
        "    else:\n",
        "        raise RuntimeError(f\"API Error {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "def load_analysis_files(folder=\"analysis_output\", max_chars=5000):\n",
        "    \"\"\"\n",
        "    Read all analysis_output files (json, md, csv, etc.)\n",
        "    and concatenate into a compact string for analysis.\n",
        "    \"\"\"\n",
        "    contents = []\n",
        "    for fname in os.listdir(folder):\n",
        "        path = os.path.join(folder, fname)\n",
        "        if os.path.isfile(path):\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read()\n",
        "                    # truncate long files\n",
        "                    if len(text) > max_chars:\n",
        "                        text = text[:max_chars] + \"\\n...[TRUNCATED]...\"\n",
        "                    contents.append(f\"## {fname}\\n{text}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {fname}: {e}\")\n",
        "    return \"\\n\".join(contents)\n",
        "\n",
        "def generate_microservices_from_analysis(analysis_text, output_dir=\"./microservices\"):\n",
        "    \"\"\"\n",
        "    Use LLaMA AI agent to propose microservice decomposition\n",
        "    based on analysis_output scripts.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert software architect.\n",
        "    Given the following static analysis reports of a monolithic codebase,\n",
        "    propose a microservices architecture.\n",
        "\n",
        "    Requirements:\n",
        "    - Identify each microservice and its core responsibilities\n",
        "    - List the main functions/classes that belong to it\n",
        "    - Specify inter-service communication (REST, events, queues, etc.)\n",
        "    - Maintain functional equivalence to the monolith\n",
        "    - Avoid redundancy and enforce consistency across services\n",
        "    - Return output in strict JSON format:\n",
        "    {{\n",
        "        \"microservices\": [\n",
        "            {{\n",
        "                \"name\": \"...\",\n",
        "                \"responsibilities\": [\"...\"],\n",
        "                \"functions\": [\"...\"],\n",
        "                \"dependencies\": [\"...\"],\n",
        "                \"api_endpoints\": [\"...\"]\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Analysis Files Content:\n",
        "    {analysis_text}\n",
        "    \"\"\"\n",
        "\n",
        "    # Query llama model\n",
        "    response = generate_gemini(prompt)\n",
        "\n",
        "    # Assuming the response is directly the text content\n",
        "    raw_output = response.strip()\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Remove the markdown code block\n",
        "        if raw_output.startswith(\"```json\"):\n",
        "            raw_output = raw_output[7:]\n",
        "        if raw_output.endswith(\"```\"):\n",
        "            raw_output = raw_output[:-3]\n",
        "        raw_output = raw_output.strip()\n",
        "\n",
        "        microservices_plan = json.loads(raw_output)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ Could not parse LLaMA output as JSON. Raw output:\")\n",
        "        print(raw_output)\n",
        "        return None\n",
        "\n",
        "    # Save microservices architecture\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    arch_file = f\"{output_dir}/microservices_plan.json\"\n",
        "    with open(arch_file, \"w\") as f:\n",
        "        json.dump(microservices_plan, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Microservices plan generated at {arch_file}\")\n",
        "    return microservices_plan\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    analysis_text = load_analysis_files(\"analysis_output\", max_chars=3000)\n",
        "    microservices_plan = generate_microservices_from_analysis(analysis_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For turning my monolithic analysis into actual scaffolded microservice code.\n",
        "\n",
        "So, In a concatenated text string of knowledge of my monolith,  I generate plan using a prompt, “Propose a microservices architecture in JSON.”\n",
        "\n",
        "Then I Extract and validate JSON.\n",
        "And save it as microservices/microservices_plan.json.\n",
        "For each service in the plan, I ask Gemini to scaffold a FastAPI project, thereby saving respective files.\n",
        "\n",
        "Then we Save the code (save_microservice_code), Parses Gemini’s response and Creates folders per service respectively.\n",
        "\n",
        "Writes app.py, requirements.txt, etc. into proper places."
      ],
      "metadata": {
        "id": "lLnNn5h5Rptw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da32f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005aceb3-8e9f-42ba-c7af-83fc88d41f1b"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Use Colab's userdata to securely access the API key\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Gemini 2.0 Flash endpoint\n",
        "BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Gemini API wrapper\n",
        "# ---------------------------\n",
        "def generate_gemini(prompt_text):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-goog-api-key\": API_KEY\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\"parts\": [{\"text\": prompt_text}]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(BASE_URL, headers=headers, json=data)\n",
        "    if response.status_code == 200:\n",
        "        resp_json = response.json()\n",
        "        if \"candidates\" in resp_json and \\\n",
        "           len(resp_json[\"candidates\"]) > 0 and \\\n",
        "           \"content\" in resp_json[\"candidates\"][0] and \\\n",
        "           \"parts\" in resp_json[\"candidates\"][0][\"content\"] and \\\n",
        "           len(resp_json[\"candidates\"][0][\"content\"][\"parts\"]) > 0 and \\\n",
        "           \"text\" in resp_json[\"candidates\"][0][\"content\"][\"parts\"][0]:\n",
        "            return resp_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise RuntimeError(f\"API response does not contain expected structure: {resp_json}\")\n",
        "    else:\n",
        "        raise RuntimeError(f\"API Error {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Load analysis files\n",
        "# ---------------------------\n",
        "def load_analysis_files(folder=\"analysis_output\", max_chars=5000):\n",
        "    contents = []\n",
        "    for fname in os.listdir(folder):\n",
        "        path = os.path.join(folder, fname)\n",
        "        if os.path.isfile(path):\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read()\n",
        "                    if len(text) > max_chars:\n",
        "                        text = text[:max_chars] + \"\\n...[TRUNCATED]...\"\n",
        "                    contents.append(f\"## {fname}\\n{text}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {fname}: {e}\")\n",
        "    return \"\\n\".join(contents)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Generate plan\n",
        "# ---------------------------\n",
        "def generate_microservices_from_analysis(analysis_text, output_dir=\"./microservices\"):\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert software architect.\n",
        "    Given the following static analysis reports of a monolithic codebase,\n",
        "    propose a microservices architecture.\n",
        "\n",
        "    Requirements:\n",
        "    - Identify each microservice and its core responsibilities\n",
        "    - List the main functions/classes that belong to it\n",
        "    - Specify inter-service communication (REST, events, queues, etc.)\n",
        "    - Maintain functional equivalence to the monolith\n",
        "    - Avoid redundancy and enforce consistency across services\n",
        "    - Return output in strict JSON format:\n",
        "    {{\n",
        "        \"microservices\": [\n",
        "            {{\n",
        "                \"name\": \"...\",\n",
        "                \"responsibilities\": [\"...\"],\n",
        "                \"functions\": [\"...\"],\n",
        "                \"dependencies\": [\"...\"],\n",
        "                \"api_endpoints\": [\"...\"]\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Analysis Files Content:\n",
        "    {analysis_text}\n",
        "    \"\"\"\n",
        "\n",
        "    raw_output = generate_gemini(prompt).strip()\n",
        "\n",
        "    try:\n",
        "        if raw_output.startswith(\"```json\"):\n",
        "            raw_output = raw_output[7:]\n",
        "        if raw_output.endswith(\"```\"):\n",
        "            raw_output = raw_output[:-3]\n",
        "        microservices_plan = json.loads(raw_output)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ Could not parse Gemini output as JSON. Raw output:\")\n",
        "        print(raw_output)\n",
        "        return None\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    arch_file = f\"{output_dir}/microservices_plan.json\"\n",
        "    with open(arch_file, \"w\") as f:\n",
        "        json.dump(microservices_plan, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Microservices plan generated at {arch_file}\")\n",
        "    return microservices_plan\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Generate code per service\n",
        "# ---------------------------\n",
        "def generate_microservice_code(service, output_dir=\"./microservices_code\"):\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert backend engineer.\n",
        "    Based on the following service definition, generate a scaffolded codebase\n",
        "    using Python FastAPI with REST endpoints.\n",
        "\n",
        "    Service definition:\n",
        "    {json.dumps(service, indent=2)}\n",
        "\n",
        "    Requirements:\n",
        "    - Create a main app.py with routes for each api_endpoint\n",
        "    - Include placeholder functions for responsibilities\n",
        "    - Add requirements.txt listing necessary libraries\n",
        "    - Organize into a modular folder structure\n",
        "    - Ensure the code runs with `uvicorn app:app --reload`\n",
        "    - Return files with this format (IMPORTANT):\n",
        "      # file: path/to/file.py\n",
        "      ```python\n",
        "      # code here\n",
        "      ```\n",
        "    \"\"\"\n",
        "\n",
        "    response = generate_gemini(prompt)\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def save_microservice_code(service_name, code_text, base_dir=\"./microservices_code\"):\n",
        "    service_dir = os.path.join(base_dir, service_name)\n",
        "    os.makedirs(service_dir, exist_ok=True)\n",
        "\n",
        "    current_file = None\n",
        "    buffers = {}\n",
        "\n",
        "    for line in code_text.splitlines():\n",
        "        clean = line.strip()\n",
        "\n",
        "        if clean.lower().startswith(\"# file:\"):\n",
        "            current_file = clean.split(\":\", 1)[-1].strip()\n",
        "            buffers[current_file] = []\n",
        "        elif clean.startswith(\"```\"):\n",
        "            continue\n",
        "        elif current_file:\n",
        "            buffers[current_file].append(line)\n",
        "\n",
        "    if not buffers:\n",
        "        buffers[\"app.py\"] = [\n",
        "            l for l in code_text.splitlines() if not l.strip().startswith(\"```\")\n",
        "        ]\n",
        "\n",
        "    for fname, lines in buffers.items():\n",
        "        path = os.path.join(service_dir, fname)\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(lines).strip() + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Saved code for {service_name} in {service_dir}\")\n",
        "\n",
        "\n",
        "def generate_code_from_plan(plan_file=\"microservices/microservices_plan.json\"):\n",
        "    with open(plan_file, \"r\") as f:\n",
        "        plan = json.load(f)\n",
        "\n",
        "    for service in plan.get(\"microservices\", []):\n",
        "        name = service[\"name\"]\n",
        "        print(f\"🚀 Generating code for service: {name}\")\n",
        "        code_text = generate_microservice_code(service)\n",
        "        if code_text:\n",
        "            save_microservice_code(name, code_text)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Run end-to-end\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    analysis_text = load_analysis_files(\"analysis_output\", max_chars=3000)\n",
        "    microservices_plan = generate_microservices_from_analysis(analysis_text)\n",
        "\n",
        "    if microservices_plan:\n",
        "        print(\"🚀 Now generating microservice code...\")\n",
        "        generate_code_from_plan(\"microservices/microservices_plan.json\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Microservices plan generated at ./microservices/microservices_plan.json\n",
            "🚀 Now generating microservice code...\n",
            "🚀 Generating code for service: CatalogService\n",
            "✅ Saved code for CatalogService in ./microservices_code/CatalogService\n",
            "🚀 Generating code for service: AccountService\n",
            "✅ Saved code for AccountService in ./microservices_code/AccountService\n",
            "🚀 Generating code for service: OrderService\n",
            "✅ Saved code for OrderService in ./microservices_code/OrderService\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXfye6RALLiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}